# LLM Workflow Orchestration Engine

A comprehensive implementation plan for building a type-safe, modular RAG orchestration and LLM inference engine using Bun, TypeScript, and Nix.

## Project Overview

This project replaces an existing n8n-based workflow system with a purpose-built, maintainable solution. The core design philosophy emphasizes:

- **Type-safe workflow composition** at compile time
- **Modular, reusable components** for tools, LLMs, retrievers, and I/O mechanisms
- **Minimal dependencies** with mature, stable libraries
- **Single-artifact deployment** via Nix flakes
- **Structured logging** with OpenTelemetry compatibility for future observability

---

## Phase 1: Project Foundation

### 1.1 Initialize Project Structure

```bash
# Create project directory
mkdir llm-orchestrator && cd llm-orchestrator

# Initialize Bun project
bun init

# Create directory structure
mkdir -p src/{core,workflows,tools,llm,retrieval,io,config}
mkdir -p src/core/{pipeline,logging,errors}
mkdir -p tests/{unit,integration}
```

### 1.2 Configure TypeScript

Create `tsconfig.json` with strict settings for maximum type safety:

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "ESNext",
    "moduleResolution": "bundler",
    "strict": true,
    "strictNullChecks": true,
    "strictFunctionTypes": true,
    "noImplicitAny": true,
    "noImplicitReturns": true,
    "noUncheckedIndexedAccess": true,
    "exactOptionalPropertyTypes": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "declaration": true,
    "declarationMap": true,
    "sourceMap": true,
    "outDir": "./dist",
    "rootDir": "./src",
    "baseUrl": ".",
    "paths": {
      "@core/*": ["src/core/*"],
      "@workflows/*": ["src/workflows/*"],
      "@tools/*": ["src/tools/*"],
      "@llm/*": ["src/llm/*"],
      "@retrieval/*": ["src/retrieval/*"],
      "@io/*": ["src/io/*"],
      "@config/*": ["src/config/*"]
    },
    "types": ["bun-types"]
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
```

### 1.3 Create Nix Flake

Create `flake.nix` for reproducible builds and deployment:

```nix
{
  description = "LLM Workflow Orchestration Engine";

  inputs = {
    nixpkgs.url = "github:nixos/nixpkgs/nixos-unstable";
    flake-utils.url = "github:numtide/flake-utils";
    bun2nix = {
      url = "github:baileyluTCD/bun2nix";
      inputs.nixpkgs.follows = "nixpkgs";
    };
  };

  outputs = { self, nixpkgs, flake-utils, bun2nix }:
    flake-utils.lib.eachDefaultSystem (system:
      let
        pkgs = nixpkgs.legacyPackages.${system};
        bun2nixLib = bun2nix.lib.${system};
      in {
        # Development shell
        devShells.default = pkgs.mkShell {
          buildInputs = with pkgs; [
            bun
            nodejs_22
            # For native dependencies if needed
            openssl
            pkg-config
          ];

          shellHook = ''
            echo "LLM Orchestrator Development Environment"
            echo "Bun version: $(bun --version)"
          '';
        };

        # Package definition
        packages.default = bun2nixLib.mkBunDerivation {
          pname = "llm-orchestrator";
          version = "0.1.0";
          src = ./.;

          bunNix = ./bun.nix;  # Generated by bun2nix

          buildPhase = ''
            bun build ./src/main.ts --compile --outfile llm-orchestrator
          '';

          installPhase = ''
            mkdir -p $out/bin
            cp llm-orchestrator $out/bin/
          '';
        };

        # NixOS module for systemd service
        nixosModules.default = { config, lib, pkgs, ... }: {
          options.services.llm-orchestrator = {
            enable = lib.mkEnableOption "LLM Orchestrator service";

            port = lib.mkOption {
              type = lib.types.port;
              default = 3000;
              description = "Port to listen on";
            };

            configFile = lib.mkOption {
              type = lib.types.path;
              description = "Path to configuration file";
            };
          };

          config = lib.mkIf config.services.llm-orchestrator.enable {
            systemd.services.llm-orchestrator = {
              description = "LLM Workflow Orchestration Engine";
              after = [ "network.target" ];
              wantedBy = [ "multi-user.target" ];

              serviceConfig = {
                ExecStart = "${self.packages.${system}.default}/bin/llm-orchestrator";
                Restart = "on-failure";
                RestartSec = 5;
                Environment = [
                  "PORT=${toString config.services.llm-orchestrator.port}"
                  "CONFIG_FILE=${config.services.llm-orchestrator.configFile}"
                ];
              };
            };
          };
        };
      });
}
```

### 1.4 Install Dependencies

Create `package.json`:

```json
{
  "name": "llm-orchestrator",
  "version": "0.1.0",
  "type": "module",
  "scripts": {
    "dev": "bun run --hot src/main.ts",
    "build": "bun build ./src/main.ts --compile --outfile dist/llm-orchestrator",
    "test": "bun test",
    "typecheck": "tsc --noEmit",
    "lint": "bunx biome check src/",
    "generate-bun-nix": "bunx bun2nix"
  },
  "dependencies": {
    "@qdrant/js-client-rest": "^1.12.0",
    "openai": "^4.77.0",
    "pino": "^9.6.0",
    "zod": "^3.24.0"
  },
  "devDependencies": {
    "@biomejs/biome": "^1.9.0",
    "@types/bun": "^1.1.0",
    "typescript": "^5.7.0"
  }
}
```

Run initial setup:

```bash
bun install
bunx bun2nix  # Generate bun.nix for Nix builds
```

---

## Phase 2: Type-Safe Pipeline System

This is the **core innovation** of the project: a compile-time type-safe workflow orchestration system.

### 2.1 Core Type Definitions

Create `src/core/pipeline/types.ts`:

```typescript
/**
 * Core pipeline types enabling compile-time type safety for workflow composition.
 *
 * Key insight: We use TypeScript's type inference to ensure that:
 * 1. Each step's input type matches the previous step's output type
 * 2. Required context is accumulated and validated at compile time
 * 3. Tool definitions are type-safe
 */

// Base result type for all pipeline steps
export type StepResult<T> = {
  success: true;
  data: T;
  metadata: StepMetadata;
} | {
  success: false;
  error: StepError;
  metadata: StepMetadata;
};

export interface StepMetadata {
  stepName: string;
  startTime: number;
  endTime: number;
  durationMs: number;
  traceId?: string;
  spanId?: string;
}

export interface StepError {
  code: string;
  message: string;
  cause?: unknown;
  retryable: boolean;
}

// Step definition - the building block of pipelines
export interface Step<TInput, TOutput, TContext = unknown> {
  name: string;
  execute: (input: TInput, context: TContext) => Promise<StepResult<TOutput>>;
  // Optional retry configuration
  retry?: {
    maxAttempts: number;
    backoffMs: number;
    retryableErrors?: string[];
  };
}

// Pipeline builder types for compile-time safety
export type PipelineStep<TIn, TOut, TCtx> = {
  step: Step<TIn, TOut, TCtx>;
  inputType: TIn;
  outputType: TOut;
};

// Extracts the output type from a step
export type StepOutput<S> = S extends Step<any, infer O, any> ? O : never;

// Extracts the input type from a step
export type StepInput<S> = S extends Step<infer I, any, any> ? I : never;

// Context accumulator type
export type MergeContext<A, B> = A & B;
```

### 2.2 Pipeline Builder Implementation

Create `src/core/pipeline/builder.ts`:

```typescript
import type { Step, StepResult, StepMetadata, StepError } from './types';
import { createLogger } from '../logging/logger';

const logger = createLogger('pipeline');

/**
 * Type-safe pipeline builder using a fluent interface.
 *
 * The key to compile-time type safety is that each `pipe()` call
 * returns a new Pipeline type that encodes both:
 * - The current output type (which becomes the next input type)
 * - The accumulated context requirements
 *
 * This means TypeScript will catch mismatched step types at compile time,
 * not runtime.
 */

// Internal representation of a pipeline stage
interface PipelineStage<TIn, TOut, TCtx> {
  step: Step<TIn, TOut, TCtx>;
}

// The Pipeline class with generic type tracking
export class Pipeline<TInput, TOutput, TContext> {
  private stages: PipelineStage<any, any, any>[] = [];
  private contextBuilder: () => TContext;

  private constructor(
    stages: PipelineStage<any, any, any>[],
    contextBuilder: () => TContext
  ) {
    this.stages = stages;
    this.contextBuilder = contextBuilder;
  }

  /**
   * Create a new pipeline starting with an initial step.
   *
   * @example
   * const pipeline = Pipeline.create(embeddingStep);
   */
  static create<I, O, C>(
    step: Step<I, O, C>,
    contextBuilder: () => C
  ): Pipeline<I, O, C> {
    return new Pipeline([{ step }], contextBuilder);
  }

  /**
   * Add a step to the pipeline.
   *
   * TypeScript enforces that NextIn === TOutput at compile time.
   * If you try to pipe a step that expects a different input type,
   * you'll get a compile error.
   *
   * @example
   * pipeline
   *   .pipe(vectorSearchStep)  // Output: SearchResults
   *   .pipe(rerankStep)        // Input must be SearchResults
   */
  pipe<NextOut, NextCtx>(
    step: Step<TOutput, NextOut, TContext & NextCtx>
  ): Pipeline<TInput, NextOut, TContext & NextCtx> {
    return new Pipeline(
      [...this.stages, { step }],
      this.contextBuilder as () => TContext & NextCtx
    );
  }

  /**
   * Add a conditional branch to the pipeline.
   * Both branches must have the same output type.
   */
  branch<BranchOut>(
    condition: (input: TOutput, context: TContext) => boolean,
    trueBranch: Step<TOutput, BranchOut, TContext>,
    falseBranch: Step<TOutput, BranchOut, TContext>
  ): Pipeline<TInput, BranchOut, TContext> {
    const branchStep: Step<TOutput, BranchOut, TContext> = {
      name: `branch(${trueBranch.name}|${falseBranch.name})`,
      execute: async (input, context) => {
        const selectedStep = condition(input, context) ? trueBranch : falseBranch;
        return selectedStep.execute(input, context);
      }
    };
    return new Pipeline(
      [...this.stages, { step: branchStep }],
      this.contextBuilder
    );
  }

  /**
   * Execute the pipeline with the given input.
   */
  async execute(input: TInput): Promise<StepResult<TOutput>> {
    const context = this.contextBuilder();
    const traceId = crypto.randomUUID();

    let currentData: any = input;

    for (const stage of this.stages) {
      const startTime = performance.now();
      const spanId = crypto.randomUUID();

      logger.info({
        event: 'step_start',
        traceId,
        spanId,
        stepName: stage.step.name,
        inputType: typeof currentData
      });

      try {
        const result = await this.executeWithRetry(
          stage.step,
          currentData,
          context,
          traceId,
          spanId
        );

        const endTime = performance.now();
        const metadata: StepMetadata = {
          stepName: stage.step.name,
          startTime,
          endTime,
          durationMs: endTime - startTime,
          traceId,
          spanId
        };

        if (!result.success) {
          logger.error({
            event: 'step_failed',
            traceId,
            spanId,
            stepName: stage.step.name,
            error: result.error,
            durationMs: metadata.durationMs
          });
          return { ...result, metadata };
        }

        logger.info({
          event: 'step_complete',
          traceId,
          spanId,
          stepName: stage.step.name,
          durationMs: metadata.durationMs
        });

        currentData = result.data;
      } catch (error) {
        const endTime = performance.now();
        const stepError: StepError = {
          code: 'UNHANDLED_ERROR',
          message: error instanceof Error ? error.message : String(error),
          cause: error,
          retryable: false
        };

        return {
          success: false,
          error: stepError,
          metadata: {
            stepName: stage.step.name,
            startTime,
            endTime,
            durationMs: endTime - startTime,
            traceId,
            spanId
          }
        };
      }
    }

    return {
      success: true,
      data: currentData as TOutput,
      metadata: {
        stepName: 'pipeline_complete',
        startTime: 0,
        endTime: performance.now(),
        durationMs: 0,
        traceId
      }
    };
  }

  private async executeWithRetry<I, O, C>(
    step: Step<I, O, C>,
    input: I,
    context: C,
    traceId: string,
    spanId: string
  ): Promise<StepResult<O>> {
    const maxAttempts = step.retry?.maxAttempts ?? 1;
    const backoffMs = step.retry?.backoffMs ?? 1000;

    let lastResult: StepResult<O> | null = null;

    for (let attempt = 1; attempt <= maxAttempts; attempt++) {
      lastResult = await step.execute(input, context);

      if (lastResult.success) {
        return lastResult;
      }

      const shouldRetry = lastResult.error.retryable &&
        attempt < maxAttempts &&
        (!step.retry?.retryableErrors ||
         step.retry.retryableErrors.includes(lastResult.error.code));

      if (!shouldRetry) {
        return lastResult;
      }

      logger.warn({
        event: 'step_retry',
        traceId,
        spanId,
        stepName: step.name,
        attempt,
        maxAttempts,
        backoffMs,
        errorCode: lastResult.error.code
      });

      await Bun.sleep(backoffMs * attempt);
    }

    return lastResult!;
  }
}
```

### 2.3 Step Factory Helpers

Create `src/core/pipeline/steps.ts`:

```typescript
import type { Step, StepResult, StepError } from './types';

/**
 * Helper functions for creating type-safe steps.
 */

export function createStep<TInput, TOutput, TContext = unknown>(
  name: string,
  execute: (input: TInput, context: TContext) => Promise<TOutput>,
  options?: {
    retry?: {
      maxAttempts: number;
      backoffMs: number;
      retryableErrors?: string[];
    };
  }
): Step<TInput, TOutput, TContext> {
  return {
    name,
    retry: options?.retry,
    execute: async (input, context): Promise<StepResult<TOutput>> => {
      try {
        const data = await execute(input, context);
        return {
          success: true,
          data,
          metadata: {
            stepName: name,
            startTime: 0,
            endTime: 0,
            durationMs: 0
          }
        };
      } catch (error) {
        const stepError: StepError = {
          code: error instanceof Error && 'code' in error
            ? String((error as any).code)
            : 'STEP_ERROR',
          message: error instanceof Error ? error.message : String(error),
          cause: error,
          retryable: isRetryableError(error)
        };
        return {
          success: false,
          error: stepError,
          metadata: {
            stepName: name,
            startTime: 0,
            endTime: 0,
            durationMs: 0
          }
        };
      }
    }
  };
}

function isRetryableError(error: unknown): boolean {
  if (error instanceof Error) {
    // Network errors, timeouts, etc.
    const retryableMessages = [
      'ECONNRESET',
      'ETIMEDOUT',
      'ECONNREFUSED',
      'fetch failed',
      'rate limit'
    ];
    return retryableMessages.some(msg =>
      error.message.toLowerCase().includes(msg.toLowerCase())
    );
  }
  return false;
}

/**
 * Create a passthrough step that transforms data without async operations.
 */
export function createTransform<TInput, TOutput>(
  name: string,
  transform: (input: TInput) => TOutput
): Step<TInput, TOutput, unknown> {
  return createStep(name, async (input) => transform(input));
}

/**
 * Create a step that runs multiple sub-steps in parallel.
 */
export function createParallel<TInput, TOutputs extends readonly unknown[]>(
  name: string,
  steps: { [K in keyof TOutputs]: Step<TInput, TOutputs[K], unknown> }
): Step<TInput, TOutputs, unknown> {
  return createStep(name, async (input) => {
    const results = await Promise.all(
      steps.map(step => step.execute(input, {}))
    );

    // Check for failures
    const failed = results.find(r => !r.success);
    if (failed && !failed.success) {
      throw new Error(`Parallel step failed: ${failed.error.message}`);
    }

    return results.map(r => r.success ? r.data : null) as unknown as TOutputs;
  });
}
```

---

## Phase 3: Structured Logging

### 3.1 Logger Implementation

Create `src/core/logging/logger.ts`:

```typescript
import pino from 'pino';

/**
 * Structured logging with Pino.
 *
 * Design decisions:
 * - JSON output by default for machine parsing
 * - OpenTelemetry-compatible fields (traceId, spanId)
 * - Minimal overhead in production
 * - Pretty printing available for development
 */

const isDev = process.env.NODE_ENV !== 'production';

// Base logger configuration
const baseLogger = pino({
  level: process.env.LOG_LEVEL || 'info',

  // Use standard OpenTelemetry field names for future compatibility
  messageKey: 'msg',
  timestamp: () => `,"time":"${new Date().toISOString()}"`,

  // Add service metadata
  base: {
    service: 'llm-orchestrator',
    version: process.env.npm_package_version || '0.0.0',
    pid: process.pid
  },

  // Pretty print in development
  transport: isDev ? {
    target: 'pino-pretty',
    options: {
      colorize: true,
      translateTime: 'HH:MM:ss.l',
      ignore: 'pid,hostname'
    }
  } : undefined,

  // Custom serializers for common objects
  serializers: {
    err: pino.stdSerializers.err,
    req: pino.stdSerializers.req,
    res: pino.stdSerializers.res
  }
});

// Type-safe child logger factory
export interface LogContext {
  traceId?: string;
  spanId?: string;
  userId?: string;
  [key: string]: unknown;
}

export type Logger = pino.Logger<never>;

export function createLogger(component: string, context?: LogContext): Logger {
  return baseLogger.child({
    component,
    ...context
  });
}

// Re-export for convenience
export { baseLogger as logger };

// Utility for adding trace context
export function withTraceContext(
  logger: Logger,
  traceId: string,
  spanId?: string
): Logger {
  return logger.child({ traceId, spanId });
}
```

---

## Phase 4: LLM Integration

### 4.1 LLM Client Abstraction

Create `src/llm/types.ts`:

```typescript
import { z } from 'zod';

/**
 * LLM abstraction layer supporting multiple backends.
 *
 * Key design principle: The interface should be backend-agnostic
 * while supporting the full feature set needed for tool calling
 * and structured output.
 */

// Message types following OpenAI conventions (widely adopted)
export type MessageRole = 'system' | 'user' | 'assistant' | 'tool';

export interface Message {
  role: MessageRole;
  content: string;
  name?: string;        // For tool messages
  toolCallId?: string;  // For tool results
}

export interface ToolCall {
  id: string;
  name: string;
  arguments: Record<string, unknown>;
}

export interface AssistantMessage extends Message {
  role: 'assistant';
  toolCalls?: ToolCall[];
}

// Tool definition
export interface ToolDefinition<TArgs = unknown> {
  name: string;
  description: string;
  parameters: z.ZodType<TArgs>;
  execute: (args: TArgs) => Promise<unknown>;
}

// Completion options
export interface CompletionOptions {
  model: string;
  messages: Message[];
  tools?: ToolDefinition[];
  toolChoice?: 'auto' | 'required' | 'none' | { name: string };
  temperature?: number;
  maxTokens?: number;
  stopSequences?: string[];
}

// Completion response
export interface CompletionResponse {
  message: AssistantMessage;
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  finishReason: 'stop' | 'tool_calls' | 'length' | 'content_filter';
}

// LLM client interface
export interface LLMClient {
  complete(options: CompletionOptions): Promise<CompletionResponse>;
  completeWithToolLoop(
    options: CompletionOptions,
    maxIterations?: number
  ): Promise<CompletionResponse>;
}
```

### 4.2 OpenAI-Compatible Client

Create `src/llm/openai-client.ts`:

```typescript
import OpenAI from 'openai';
import type {
  LLMClient,
  CompletionOptions,
  CompletionResponse,
  Message,
  AssistantMessage,
  ToolCall,
  ToolDefinition
} from './types';
import { createLogger } from '../core/logging/logger';
import { zodToJsonSchema } from 'zod-to-json-schema';

const logger = createLogger('llm-client');

/**
 * OpenAI-compatible LLM client.
 *
 * Works with:
 * - OpenAI API
 * - llama.cpp server (with --api-key flag)
 * - vLLM
 * - Any OpenAI-compatible endpoint
 */
export class OpenAICompatibleClient implements LLMClient {
  private client: OpenAI;

  constructor(options: {
    baseURL: string;
    apiKey?: string;
    timeout?: number;
  }) {
    this.client = new OpenAI({
      baseURL: options.baseURL,
      apiKey: options.apiKey || 'not-required',
      timeout: options.timeout || 120000,
      maxRetries: 3
    });
  }

  async complete(options: CompletionOptions): Promise<CompletionResponse> {
    const tools = options.tools?.map(tool => ({
      type: 'function' as const,
      function: {
        name: tool.name,
        description: tool.description,
        parameters: zodToJsonSchema(tool.parameters)
      }
    }));

    const response = await this.client.chat.completions.create({
      model: options.model,
      messages: this.convertMessages(options.messages),
      tools: tools?.length ? tools : undefined,
      tool_choice: this.convertToolChoice(options.toolChoice),
      temperature: options.temperature ?? 0.7,
      max_tokens: options.maxTokens,
      stop: options.stopSequences
    });

    const choice = response.choices[0];
    const message = choice.message;

    const assistantMessage: AssistantMessage = {
      role: 'assistant',
      content: message.content || '',
      toolCalls: message.tool_calls?.map(tc => ({
        id: tc.id,
        name: tc.function.name,
        arguments: JSON.parse(tc.function.arguments)
      }))
    };

    return {
      message: assistantMessage,
      usage: {
        promptTokens: response.usage?.prompt_tokens || 0,
        completionTokens: response.usage?.completion_tokens || 0,
        totalTokens: response.usage?.total_tokens || 0
      },
      finishReason: this.mapFinishReason(choice.finish_reason)
    };
  }

  async completeWithToolLoop(
    options: CompletionOptions,
    maxIterations: number = 10
  ): Promise<CompletionResponse> {
    const messages = [...options.messages];
    let lastResponse: CompletionResponse | null = null;

    for (let i = 0; i < maxIterations; i++) {
      logger.debug({
        event: 'tool_loop_iteration',
        iteration: i + 1,
        maxIterations,
        messageCount: messages.length
      });

      lastResponse = await this.complete({
        ...options,
        messages
      });

      // If no tool calls, we're done
      if (!lastResponse.message.toolCalls?.length) {
        return lastResponse;
      }

      // Add assistant message with tool calls
      messages.push(lastResponse.message);

      // Execute tools and add results
      for (const toolCall of lastResponse.message.toolCalls) {
        const tool = options.tools?.find(t => t.name === toolCall.name);

        if (!tool) {
          logger.error({
            event: 'unknown_tool_call',
            toolName: toolCall.name
          });
          messages.push({
            role: 'tool',
            content: JSON.stringify({ error: `Unknown tool: ${toolCall.name}` }),
            toolCallId: toolCall.id
          });
          continue;
        }

        try {
          // Validate arguments against schema
          const validatedArgs = tool.parameters.parse(toolCall.arguments);

          logger.info({
            event: 'tool_execution_start',
            toolName: tool.name,
            arguments: validatedArgs
          });

          const result = await tool.execute(validatedArgs);

          logger.info({
            event: 'tool_execution_complete',
            toolName: tool.name,
            resultType: typeof result
          });

          messages.push({
            role: 'tool',
            content: JSON.stringify(result),
            toolCallId: toolCall.id
          });
        } catch (error) {
          logger.error({
            event: 'tool_execution_error',
            toolName: tool.name,
            error: error instanceof Error ? error.message : String(error)
          });
          messages.push({
            role: 'tool',
            content: JSON.stringify({
              error: error instanceof Error ? error.message : String(error)
            }),
            toolCallId: toolCall.id
          });
        }
      }
    }

    logger.warn({
      event: 'tool_loop_max_iterations',
      maxIterations
    });

    return lastResponse!;
  }

  private convertMessages(messages: Message[]): OpenAI.Chat.ChatCompletionMessageParam[] {
    return messages.map(msg => {
      switch (msg.role) {
        case 'system':
          return { role: 'system', content: msg.content };
        case 'user':
          return { role: 'user', content: msg.content };
        case 'assistant':
          return {
            role: 'assistant',
            content: msg.content,
            tool_calls: (msg as AssistantMessage).toolCalls?.map(tc => ({
              id: tc.id,
              type: 'function' as const,
              function: {
                name: tc.name,
                arguments: JSON.stringify(tc.arguments)
              }
            }))
          };
        case 'tool':
          return {
            role: 'tool',
            content: msg.content,
            tool_call_id: msg.toolCallId!
          };
        default:
          throw new Error(`Unknown message role: ${msg.role}`);
      }
    });
  }

  private convertToolChoice(
    choice?: CompletionOptions['toolChoice']
  ): OpenAI.Chat.ChatCompletionToolChoiceOption | undefined {
    if (!choice) return undefined;
    if (typeof choice === 'string') return choice;
    return { type: 'function', function: { name: choice.name } };
  }

  private mapFinishReason(
    reason: string | null
  ): CompletionResponse['finishReason'] {
    switch (reason) {
      case 'stop': return 'stop';
      case 'tool_calls': return 'tool_calls';
      case 'length': return 'length';
      case 'content_filter': return 'content_filter';
      default: return 'stop';
    }
  }
}
```

---

## Phase 5: Vector Search Integration

### 5.1 Qdrant Client Wrapper

Create `src/retrieval/qdrant-client.ts`:

```typescript
import { QdrantClient } from '@qdrant/js-client-rest';
import type { Filter, PointStruct } from '@qdrant/js-client-rest';
import { createLogger } from '../core/logging/logger';

const logger = createLogger('qdrant');

export interface SearchOptions {
  query: string;
  collection: string;
  limit?: number;
  scoreThreshold?: number;
  filter?: Filter;
  withPayload?: boolean | string[];
}

export interface SearchResult {
  id: string | number;
  score: number;
  payload: Record<string, unknown>;
}

export interface QdrantConfig {
  url: string;
  apiKey?: string;
}

/**
 * Qdrant vector search client wrapper.
 *
 * Note: This client assumes embeddings are generated externally
 * and passed in. For embedding generation, see the embedding step.
 */
export class VectorSearchClient {
  private client: QdrantClient;

  constructor(config: QdrantConfig) {
    this.client = new QdrantClient({
      url: config.url,
      apiKey: config.apiKey
    });
  }

  async search(
    vector: number[],
    options: Omit<SearchOptions, 'query'>
  ): Promise<SearchResult[]> {
    const startTime = performance.now();

    logger.debug({
      event: 'vector_search_start',
      collection: options.collection,
      limit: options.limit,
      vectorDim: vector.length,
      hasFilter: !!options.filter
    });

    try {
      const results = await this.client.search(options.collection, {
        vector,
        limit: options.limit || 10,
        score_threshold: options.scoreThreshold,
        filter: options.filter,
        with_payload: options.withPayload ?? true
      });

      const durationMs = performance.now() - startTime;

      logger.info({
        event: 'vector_search_complete',
        collection: options.collection,
        resultCount: results.length,
        durationMs,
        topScore: results[0]?.score
      });

      return results.map(r => ({
        id: r.id,
        score: r.score,
        payload: r.payload as Record<string, unknown>
      }));
    } catch (error) {
      logger.error({
        event: 'vector_search_error',
        collection: options.collection,
        error: error instanceof Error ? error.message : String(error)
      });
      throw error;
    }
  }

  async searchWithMetadataFilter(
    vector: number[],
    collection: string,
    filters: {
      must?: Array<{ key: string; match: { value: string | number | boolean } }>;
      should?: Array<{ key: string; match: { value: string | number | boolean } }>;
      mustNot?: Array<{ key: string; match: { value: string | number | boolean } }>;
    },
    options?: {
      limit?: number;
      scoreThreshold?: number;
    }
  ): Promise<SearchResult[]> {
    const filter: Filter = {};

    if (filters.must?.length) {
      filter.must = filters.must.map(f => ({
        key: f.key,
        match: f.match
      }));
    }

    if (filters.should?.length) {
      filter.should = filters.should.map(f => ({
        key: f.key,
        match: f.match
      }));
    }

    if (filters.mustNot?.length) {
      filter.must_not = filters.mustNot.map(f => ({
        key: f.key,
        match: f.match
      }));
    }

    return this.search(vector, {
      collection,
      filter,
      limit: options?.limit,
      scoreThreshold: options?.scoreThreshold
    });
  }

  async getCollectionInfo(collection: string) {
    return this.client.getCollection(collection);
  }
}
```

### 5.2 Embedding Generation Step

Create `src/retrieval/embedding.ts`:

```typescript
import { createStep } from '../core/pipeline/steps';
import type { LLMClient } from '../llm/types';
import { createLogger } from '../core/logging/logger';

const logger = createLogger('embedding');

/**
 * Embedding generation step.
 *
 * Supports multiple backends through the OpenAI-compatible interface:
 * - llama.cpp with embedding models
 * - Local sentence-transformers via a wrapper
 * - OpenAI embeddings API
 */

export interface EmbeddingConfig {
  baseURL: string;
  model: string;
  apiKey?: string;
}

export interface EmbeddingInput {
  text: string;
  metadata?: Record<string, unknown>;
}

export interface EmbeddingOutput {
  embedding: number[];
  text: string;
  metadata?: Record<string, unknown>;
}

export function createEmbeddingStep(config: EmbeddingConfig) {
  return createStep<EmbeddingInput, EmbeddingOutput>(
    'generate_embedding',
    async (input) => {
      const response = await fetch(`${config.baseURL}/embeddings`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          ...(config.apiKey && { 'Authorization': `Bearer ${config.apiKey}` })
        },
        body: JSON.stringify({
          model: config.model,
          input: input.text
        })
      });

      if (!response.ok) {
        throw new Error(`Embedding request failed: ${response.statusText}`);
      }

      const data = await response.json() as {
        data: Array<{ embedding: number[] }>;
      };

      logger.debug({
        event: 'embedding_generated',
        textLength: input.text.length,
        embeddingDim: data.data[0].embedding.length
      });

      return {
        embedding: data.data[0].embedding,
        text: input.text,
        metadata: input.metadata
      };
    },
    {
      retry: {
        maxAttempts: 3,
        backoffMs: 1000,
        retryableErrors: ['ECONNRESET', 'ETIMEDOUT']
      }
    }
  );
}
```

---

## Phase 6: Tool Definitions

### 6.1 Tool Registry

Create `src/tools/registry.ts`:

```typescript
import { z } from 'zod';
import type { ToolDefinition } from '../llm/types';

/**
 * Type-safe tool registry.
 *
 * Tools are defined with Zod schemas for:
 * 1. Runtime argument validation
 * 2. JSON Schema generation for LLM tool descriptions
 * 3. TypeScript type inference for execute functions
 */

export class ToolRegistry {
  private tools = new Map<string, ToolDefinition>();

  register<TArgs>(tool: ToolDefinition<TArgs>): this {
    if (this.tools.has(tool.name)) {
      throw new Error(`Tool already registered: ${tool.name}`);
    }
    this.tools.set(tool.name, tool as ToolDefinition);
    return this;
  }

  get(name: string): ToolDefinition | undefined {
    return this.tools.get(name);
  }

  getAll(): ToolDefinition[] {
    return Array.from(this.tools.values());
  }

  has(name: string): boolean {
    return this.tools.has(name);
  }
}

// Factory function for creating tools with full type inference
export function defineTool<TArgs>(
  definition: ToolDefinition<TArgs>
): ToolDefinition<TArgs> {
  return definition;
}
```

### 6.2 RAG Search Tool

Create `src/tools/rag-search.ts`:

```typescript
import { z } from 'zod';
import { defineTool } from './registry';
import type { VectorSearchClient, SearchResult } from '../retrieval/qdrant-client';
import type { EmbeddingConfig } from '../retrieval/embedding';
import { createLogger } from '../core/logging/logger';

const logger = createLogger('rag-tool');

export interface RAGSearchContext {
  vectorClient: VectorSearchClient;
  embeddingConfig: EmbeddingConfig;
  defaultCollection: string;
}

const searchArgsSchema = z.object({
  query: z.string().describe('The search query to find relevant documents'),
  collection: z.string().optional().describe('Collection to search (optional, uses default if not specified)'),
  limit: z.number().optional().default(5).describe('Maximum number of results to return'),
  tags: z.array(z.string()).optional().describe('Filter results by tags')
});

type SearchArgs = z.infer<typeof searchArgsSchema>;

export function createRAGSearchTool(context: RAGSearchContext) {
  return defineTool({
    name: 'search_knowledge_base',
    description: 'Search the knowledge base for relevant documents and notes. Use this to find information related to a user query.',
    parameters: searchArgsSchema,
    execute: async (args: SearchArgs): Promise<SearchResult[]> => {
      logger.info({
        event: 'rag_search_start',
        query: args.query,
        collection: args.collection || context.defaultCollection,
        limit: args.limit
      });

      // Generate embedding for query
      const embeddingResponse = await fetch(
        `${context.embeddingConfig.baseURL}/embeddings`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            ...(context.embeddingConfig.apiKey && {
              'Authorization': `Bearer ${context.embeddingConfig.apiKey}`
            })
          },
          body: JSON.stringify({
            model: context.embeddingConfig.model,
            input: args.query
          })
        }
      );

      if (!embeddingResponse.ok) {
        throw new Error(`Embedding generation failed: ${embeddingResponse.statusText}`);
      }

      const embeddingData = await embeddingResponse.json() as {
        data: Array<{ embedding: number[] }>;
      };

      // Search with optional tag filter
      const filters = args.tags?.length ? {
        should: args.tags.map(tag => ({
          key: 'tags',
          match: { value: tag }
        }))
      } : undefined;

      const results = await context.vectorClient.searchWithMetadataFilter(
        embeddingData.data[0].embedding,
        args.collection || context.defaultCollection,
        filters || {},
        { limit: args.limit }
      );

      logger.info({
        event: 'rag_search_complete',
        resultCount: results.length,
        topScore: results[0]?.score
      });

      return results;
    }
  });
}
```

---

## Phase 7: CLI Interface

### 7.1 CLI Entry Point

Create `src/io/cli.ts`:

```typescript
import { parseArgs } from 'util';
import { createLogger } from '../core/logging/logger';
import type { LLMClient } from '../llm/types';
import type { ToolDefinition } from '../llm/types';

const logger = createLogger('cli');

export interface CLIOptions {
  llmClient: LLMClient;
  tools: ToolDefinition[];
  systemPrompt?: string;
  model: string;
}

export async function runCLI(options: CLIOptions) {
  const { values } = parseArgs({
    args: process.argv.slice(2),
    options: {
      query: { type: 'string', short: 'q' },
      interactive: { type: 'boolean', short: 'i', default: false },
      verbose: { type: 'boolean', short: 'v', default: false }
    },
    strict: true
  });

  if (values.verbose) {
    process.env.LOG_LEVEL = 'debug';
  }

  const systemPrompt = options.systemPrompt || `You are a helpful assistant with access to a knowledge base. Use the available tools to answer user questions accurately.`;

  if (values.query) {
    // Single query mode
    await processQuery(options, systemPrompt, values.query);
  } else if (values.interactive) {
    // Interactive mode
    await runInteractive(options, systemPrompt);
  } else {
    // Read from stdin
    const input = await Bun.stdin.text();
    await processQuery(options, systemPrompt, input.trim());
  }
}

async function processQuery(
  options: CLIOptions,
  systemPrompt: string,
  query: string
) {
  logger.info({ event: 'query_start', query });

  const startTime = performance.now();

  const response = await options.llmClient.completeWithToolLoop({
    model: options.model,
    messages: [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: query }
    ],
    tools: options.tools,
    toolChoice: 'auto'
  });

  const durationMs = performance.now() - startTime;

  logger.info({
    event: 'query_complete',
    durationMs,
    finishReason: response.finishReason,
    usage: response.usage
  });

  console.log('\n' + response.message.content);
}

async function runInteractive(
  options: CLIOptions,
  systemPrompt: string
) {
  const messages = [
    { role: 'system' as const, content: systemPrompt }
  ];

  console.log('Interactive mode. Type "exit" to quit.\n');

  const reader = Bun.stdin.stream().getReader();
  const decoder = new TextDecoder();

  process.stdout.write('> ');

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const input = decoder.decode(value).trim();

    if (input.toLowerCase() === 'exit') {
      console.log('Goodbye!');
      break;
    }

    if (!input) {
      process.stdout.write('> ');
      continue;
    }

    messages.push({ role: 'user', content: input });

    const response = await options.llmClient.completeWithToolLoop({
      model: options.model,
      messages,
      tools: options.tools,
      toolChoice: 'auto'
    });

    messages.push(response.message);

    console.log('\n' + response.message.content + '\n');
    process.stdout.write('> ');
  }
}
```

---

## Phase 8: Main Application

### 8.1 Configuration

Create `src/config/schema.ts`:

```typescript
import { z } from 'zod';

export const configSchema = z.object({
  server: z.object({
    port: z.number().default(3000),
    host: z.string().default('0.0.0.0')
  }).default({}),

  llm: z.object({
    baseURL: z.string(),
    apiKey: z.string().optional(),
    model: z.string().default('qwen3:30b'),
    timeout: z.number().default(120000)
  }),

  embedding: z.object({
    baseURL: z.string(),
    model: z.string(),
    apiKey: z.string().optional()
  }),

  qdrant: z.object({
    url: z.string(),
    apiKey: z.string().optional(),
    defaultCollection: z.string().default('obsidian-notes')
  }),

  logging: z.object({
    level: z.enum(['trace', 'debug', 'info', 'warn', 'error', 'fatal']).default('info'),
    pretty: z.boolean().default(false)
  }).default({})
});

export type Config = z.infer<typeof configSchema>;

export async function loadConfig(path?: string): Promise<Config> {
  const configPath = path || process.env.CONFIG_FILE || './config.json';

  try {
    const file = Bun.file(configPath);
    const raw = await file.json();
    return configSchema.parse(raw);
  } catch (error) {
    // Try environment variables as fallback
    return configSchema.parse({
      llm: {
        baseURL: process.env.LLM_BASE_URL,
        apiKey: process.env.LLM_API_KEY,
        model: process.env.LLM_MODEL
      },
      embedding: {
        baseURL: process.env.EMBEDDING_BASE_URL,
        model: process.env.EMBEDDING_MODEL,
        apiKey: process.env.EMBEDDING_API_KEY
      },
      qdrant: {
        url: process.env.QDRANT_URL,
        apiKey: process.env.QDRANT_API_KEY,
        defaultCollection: process.env.QDRANT_COLLECTION
      }
    });
  }
}
```

### 8.2 Main Entry Point

Create `src/main.ts`:

```typescript
import { loadConfig } from './config/schema';
import { OpenAICompatibleClient } from './llm/openai-client';
import { VectorSearchClient } from './retrieval/qdrant-client';
import { ToolRegistry } from './tools/registry';
import { createRAGSearchTool } from './tools/rag-search';
import { runCLI } from './io/cli';
import { createLogger } from './core/logging/logger';

const logger = createLogger('main');

async function main() {
  logger.info({ event: 'startup' });

  // Load configuration
  const config = await loadConfig();

  // Initialize clients
  const llmClient = new OpenAICompatibleClient({
    baseURL: config.llm.baseURL,
    apiKey: config.llm.apiKey,
    timeout: config.llm.timeout
  });

  const vectorClient = new VectorSearchClient({
    url: config.qdrant.url,
    apiKey: config.qdrant.apiKey
  });

  // Register tools
  const toolRegistry = new ToolRegistry();

  const ragSearchTool = createRAGSearchTool({
    vectorClient,
    embeddingConfig: config.embedding,
    defaultCollection: config.qdrant.defaultCollection
  });

  toolRegistry.register(ragSearchTool);

  // Run CLI
  await runCLI({
    llmClient,
    tools: toolRegistry.getAll(),
    model: config.llm.model,
    systemPrompt: `You are a helpful assistant with access to a personal knowledge base.

When asked a question:
1. Use the search_knowledge_base tool to find relevant information
2. Synthesize the information into a clear, helpful response
3. Cite sources when relevant

Be concise but thorough in your responses.`
  });
}

main().catch(error => {
  logger.fatal({ event: 'fatal_error', error });
  process.exit(1);
});
```

---

## Phase 9: HTTP Server (Optional Extension)

Create `src/io/server.ts`:

```typescript
import type { Config } from '../config/schema';
import type { LLMClient } from '../llm/types';
import type { ToolDefinition } from '../llm/types';
import { createLogger } from '../core/logging/logger';

const logger = createLogger('server');

export interface ServerOptions {
  config: Config;
  llmClient: LLMClient;
  tools: ToolDefinition[];
}

export function createServer(options: ServerOptions) {
  const { config, llmClient, tools } = options;

  return Bun.serve({
    port: config.server.port,
    hostname: config.server.host,

    routes: {
      // Health check
      '/health': new Response('OK'),

      // Query endpoint
      '/api/query': {
        POST: async (req) => {
          const body = await req.json() as { query: string };

          if (!body.query) {
            return Response.json(
              { error: 'Missing query parameter' },
              { status: 400 }
            );
          }

          try {
            const response = await llmClient.completeWithToolLoop({
              model: config.llm.model,
              messages: [
                { role: 'system', content: 'You are a helpful assistant.' },
                { role: 'user', content: body.query }
              ],
              tools,
              toolChoice: 'auto'
            });

            return Response.json({
              response: response.message.content,
              usage: response.usage
            });
          } catch (error) {
            logger.error({
              event: 'query_error',
              error: error instanceof Error ? error.message : String(error)
            });
            return Response.json(
              { error: 'Query processing failed' },
              { status: 500 }
            );
          }
        }
      }
    },

    fetch(req) {
      return new Response('Not Found', { status: 404 });
    }
  });
}
```

---

## Implementation Order for Claude Code

Execute these phases in order. Each phase builds on the previous ones.

### Step 1: Initialize Project (Execute First)

```bash
# Commands to run
mkdir llm-orchestrator && cd llm-orchestrator
bun init -y
mkdir -p src/{core/{pipeline,logging,errors},workflows,tools,llm,retrieval,io,config}
mkdir -p tests/{unit,integration}
```

Then create `tsconfig.json`, `package.json`, and `flake.nix` as specified in Phase 1.

### Step 2: Install Dependencies

```bash
bun add @qdrant/js-client-rest openai pino zod zod-to-json-schema
bun add -d @biomejs/biome @types/bun typescript pino-pretty
```

### Step 3: Core Pipeline System

Create files in this order:
1. `src/core/pipeline/types.ts`
2. `src/core/logging/logger.ts`
3. `src/core/pipeline/builder.ts`
4. `src/core/pipeline/steps.ts`

### Step 4: LLM Integration

Create files in this order:
1. `src/llm/types.ts`
2. `src/llm/openai-client.ts`

### Step 5: Vector Search

Create files in this order:
1. `src/retrieval/qdrant-client.ts`
2. `src/retrieval/embedding.ts`

### Step 6: Tools

Create files in this order:
1. `src/tools/registry.ts`
2. `src/tools/rag-search.ts`

### Step 7: I/O and Configuration

Create files in this order:
1. `src/config/schema.ts`
2. `src/io/cli.ts`
3. `src/main.ts`

### Step 8: Testing

Create example config file `config.example.json`:

```json
{
  "llm": {
    "baseURL": "http://localhost:8080/v1",
    "model": "qwen3:30b"
  },
  "embedding": {
    "baseURL": "http://localhost:8080/v1",
    "model": "nomic-embed-text"
  },
  "qdrant": {
    "url": "http://localhost:6333",
    "defaultCollection": "obsidian-notes"
  },
  "logging": {
    "level": "info",
    "pretty": true
  }
}
```

### Step 9: Generate Nix Lock

```bash
bunx bun2nix
```

---

## Future Extensions

This architecture supports easy addition of:

### Additional Tools
- File system tools (read/write files)
- Web search tools
- Calendar/scheduling tools
- Custom API integrations

### Additional I/O Mechanisms
- Telegram bot integration
- MCP server implementation
- Webhook handlers
- WebSocket real-time interface

### Additional LLM Backends
- Anthropic Claude API
- Local models via different servers
- Fallback/routing between models

### Workflow Templates
- Multi-step research workflows
- Document summarization pipelines
- Question-answering with citations

---

## Key Design Decisions Summary

| Decision | Rationale |
|----------|-----------|
| Type-safe pipeline builder | Catch workflow composition errors at compile time, not runtime |
| Zod for tool schemas | Runtime validation + JSON Schema generation + TypeScript inference |
| Pino for logging | High performance, JSON output, OpenTelemetry compatible |
| OpenAI-compatible client | Works with local llama.cpp, vLLM, and OpenAI |
| Nix flakes for deployment | Reproducible builds, single-artifact deployment |
| Bun for runtime | Fast startup, native TypeScript, built-in HTTP server |
| Minimal dependencies | Long-term maintainability, less churn |

---

## Testing Strategy

### Unit Tests
- Pipeline step composition
- Tool argument validation
- Configuration parsing

### Integration Tests
- LLM client with mock server
- Qdrant client with test collection
- Full query flow

### Manual Testing Checklist
- [ ] CLI single query mode
- [ ] CLI interactive mode
- [ ] Tool execution with real LLM
- [ ] Vector search with real Qdrant
- [ ] Error handling and retries
- [ ] Logging output verification
